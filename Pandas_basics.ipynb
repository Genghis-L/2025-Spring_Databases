{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the notebook collection for Pandas basics. \n",
    "---\n",
    "Remark: I only upload knowledge I may forget\n",
    "\n",
    "## Table of Contents\n",
    "1. [Pandas Series](#pandas-series)\n",
    "2. [Pandas Dataframe](#pandas-dataframe)\n",
    "3. [Company Data example](#company-data-example)\n",
    "4. [City Rainfall example](#city-rainfall-example)\n",
    "5. [Imputation](#imputation)\n",
    "6. [Binning](#binning)\n",
    "7. [Reformatting](#reformatting)\n",
    "8. [Merging](#merging)\n",
    "9. [Indexes, Hierarchical Indexes, and Grouping](#indexes-hierarchical-indexes-and-grouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Series\n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating Pandas series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# without the index specified, you get numerical index values starting with zero\n",
    "series =pd.Series([7,8,9])\n",
    "series2 =pd.Series(np.array([7,8,9])) # equivalent\n",
    "print(series)\n",
    "\n",
    "print('\\n', '-'*30, '\\n')\n",
    "\n",
    "# unlike numpy, data can be mixed type\n",
    "series3=pd.Series([\"Batman\",\"Spiderman\",42])\n",
    "print(series3)\n",
    "\n",
    "print('\\n', '-'*30, '\\n')\n",
    "\n",
    "# We can set a custom index and it is not unique\n",
    "cities=pd.Series([\"Stamford\",\"Albany\",\"Buffalo\",\"Hartford\"],[\"CT\",\"NY\",\"NY\",\"CT\"])\n",
    "# We can use keywords equivalently\n",
    "cities2=pd.Series(data=[\"Stamford\",\"Albany\",\"Buffalo\",\"Hartford\"],index=[\"CT\",\"NY\",\"NY\",\"CT\"])\n",
    "# We can use a dictionary equivalently but index will be uniquely determined by the last key\n",
    "cities3=pd.Series({\"CT\":\"Stamford\",\"NY\":\"Albany\",\"NY\":\"Buffalo\",\"CT\":\"Hartford\"})\n",
    "print(cities, '\\n')\n",
    "print(cities['NY'], '\\n')\n",
    "print(cities3, '\\n')\n",
    "print(cities3['NY'],type(cities3['NY']))    # when we access a single element, it is its value instead of a series object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nuances in dictionary assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# if a key from the dictionary data is not listed in the index, its value is not included in the Series.\n",
    "series=pd.Series({'A':'ant', 'B':'bat'},['A'])\n",
    "print(series)\n",
    "\n",
    "print('\\n', '-'*30, '\\n')\n",
    "\n",
    "# if an index label is not in the dictionary, it is included as NaN missing data.\n",
    "x=pd.Series({'A':'ant','B':'bat'},['A','C'])\n",
    "print(x, '\\n')\n",
    "print(pd.isnull(x),'\\n')\n",
    "print(pd.notnull(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s=pd.Series(data=[7,8,9],index=['x','y','z'])\n",
    "print(\"Series values:\",s.values)\n",
    "print(type(s.values)) # the values are stored in a NumPy ndarray\n",
    "print(\"Series index:\",s.index)\n",
    "print(type(s.index)) # the index is stored in its own type of object\n",
    "print(list(s.index)) # but it can be converted to an ordinary list\n",
    "\n",
    "print('\\n', '-'*30, '\\n')\n",
    "\n",
    "print(s)\n",
    "print(s['y'],type(s['y'])) # single value\n",
    "print(s[1]) # equivalent as you can either use index value or the numerical row number\n",
    "print(s.iloc[1]) # equivalent\n",
    "\n",
    "print(s['y':'z']) \n",
    "print(s[['y','y','z']]) # multiple values as a new duplicated series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s1=pd.Series([1,2],['x','y'])\n",
    "s2=pd.Series([6,3],['x','z'])\n",
    "# for addition, labels must be present in both Series\n",
    "print(s1+s2)\n",
    "\n",
    "s3=pd.Series([2,3],['x','y'])\n",
    "print(s1*s3,'\\n',s1+s3,'\\n',s1-s3,'\\n',s1/s3, '\\n',s1%s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- comparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need identical labels to compare element-wise\n",
    "s1=pd.Series([1,2],['x','y'])\n",
    "s2=pd.Series([1,9],['x','z'])\n",
    "try:\n",
    "  s1==s2\n",
    "except ValueError as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "s = pd.Series([2, 3, 4, 5])\n",
    "# you can filter with a Boolean list\n",
    "print(s[[True, False, True, False]])\n",
    "# or can filter with an expression\n",
    "print(s[s%2!=1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Dataframe\n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- creating dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe=pd.DataFrame(data=[[1, 2, 3], [4, 5, 6], [7, 8, 9]], index=['r1', 'r2', 'r3'], columns=['A', 'B', 'C'])\n",
    "print(dataframe)\n",
    "\n",
    "# we can use a nested dictionary equivalently\n",
    "dataframe2=pd.DataFrame({'A': {'r1': 1, 'r2': 4, 'r3': 7}, \n",
    "                         'B': {'r1': 2, 'r2': 5, 'r3': 8}, \n",
    "                         'C': {'r1': 3, 'r2': 6, 'r3': 9}})\n",
    "print(dataframe2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataframe attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# here index is left out so is generated automatically\n",
    "x=pd.DataFrame([[1, 2, 3], ['a', 5, 6]],\n",
    "    columns=['A', 'B', 'C'])\n",
    "print(x)\n",
    "print(x.T)  # transpose\n",
    "\n",
    "print(x.values)\n",
    "print(type(x.values)) # NumPy type\n",
    "\n",
    "print(x.index, list(x.index), x.index[1]) \n",
    "\n",
    "try:\n",
    "    x.index[1] = 'D' # error: index is immutable\n",
    "except TypeError as e:\n",
    "    print(e)\n",
    "\n",
    "print(x.columns, list(x.columns), x.columns[1])\n",
    "\n",
    "try:\n",
    "    x.columns[1] = 'D' # error: columns are immutable\n",
    "except TypeError as e:\n",
    "    print(e)\n",
    "    \n",
    "print(x.dtypes) # data types of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- accessing columns and elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x=pd.DataFrame([[1, 2, 3], ['a', 5, 6]],\n",
    "    columns=['A', 'B', 'C'])\n",
    "print(x)\n",
    "\n",
    "print(x['A']) # first column\n",
    "print(x.A.name) # column name\n",
    "print(type(x['A'])) # Pandas Series type\n",
    "print(x[['C','A','C']]) # select, reorder and duplicate multiple columns\n",
    "print(type(x[['C','A','C']])) # Pandas DataFrame type\n",
    "print(x.loc[:, 'B':'C'])    # column slicing\n",
    "\n",
    "# Caveat: we can not use x['A':'C'] to extract multiple columns as in NumPy, it is only for rows\n",
    "\n",
    "print(x['A'][0]) # first value in first column, caveat: we can not use x['A',0] as in NumPy\n",
    "print(type(x['A'][0])) # NumPy type integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- accessing rows and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.DataFrame({\"cA\": {'r1': 1, 'r2': 2, 'r3': 3},\n",
    "                  \"cB\": {'r1': 4, 'r2': 5, 'r3': 6},\n",
    "                  \"cC\": {'r1': 7, 'r2': 8, 'r3': 9}})\n",
    "d = pd.DataFrame(index=['r1', 'r2', 'r3'], data=[[1,4,7],[2,5,8],[3,6,9]], columns=['cA', 'cB', 'cC'])\n",
    "print(d, '\\n')\n",
    "\n",
    "# first row\n",
    "print(d.iloc[0], '\\n')\n",
    "print(d.loc['r1'], '\\n') # equivalent, row slicing\n",
    "print(type(d.loc['r1']), '\\n') # Pandas Series type\n",
    "\n",
    "# last two rows\n",
    "print(d[1:3], '\\n') \n",
    "print(type(d[1:3]), '\\n') # Pandas DataFrame type\n",
    "print(d['r2':'r3'], '\\n')   # equivalent\n",
    "print(d[[False,True,True]], '\\n')  # equivalent\n",
    "print(d[d['cA']!=1], '\\n')    # equivalent by filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reassignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.DataFrame(index=['r1', 'r2', 'r3'], data=[[1,4,7],[2,5,8],[3,6,9]], columns=['cA', 'cB', 'cC'])\n",
    "print(d, '\\n')\n",
    "\n",
    "d['cB'] = 99    # reassign whole column\n",
    "d['cB'] = [99,99,99]    # equivalent by list\n",
    "d['cB'] = pd.Series([99,99,99], index=['r1','r2','r3'])    # equivalent by Series\n",
    "# since index is string, we need to specify, otheriwse it will be NaN\n",
    "\n",
    "print(d, '\\n')\n",
    "\n",
    "d['cB'] = pd.Series([99,99,99], index=['r1','r2','r4'])    # missing values handled gracefully\n",
    "\n",
    "print(d, '\\n')\n",
    "\n",
    "print('\\n', '-'*30, '\\n')\n",
    "\n",
    "d = pd.DataFrame(index=['r1', 'r2', 'r3'], data=[[1,4,7],[2,5,8],[3,6,9]], columns=['cA', 'cB', 'cC'])\n",
    "print(d, '\\n')\n",
    "\n",
    "# Reassign by filtering\n",
    "d[d['cA'] > 1] = 0  # whole rows\n",
    "d.loc[d['cA'] > 0, 'cB'] = 99  # single value\n",
    "\n",
    "print(d, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "df = pd.DataFrame(np.arange(9).reshape((3, 3)),\n",
    "                 columns = ['a', 'b', 'c'])\n",
    "print(df, '\\n')\n",
    "\n",
    "# Uppercase column names\n",
    "df.rename(columns=str.upper,inplace=True)\n",
    "df.columns=df.columns.map(str.upper)    # equivalent\n",
    "print(df, '\\n')\n",
    "\n",
    "# Changing column names\n",
    "df.index=[5,7,9]\n",
    "df.columns=[\"Pluto\", \"Mickey\", \"Goofy\"]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- column dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([[4, 5, 6], [7, 8, 9]],\n",
    "    columns=['foo', 'bar', 'baz'])\n",
    "print(df)\n",
    "\n",
    "df['qux'] = [20, 30]  # add new column\n",
    "\n",
    "# need to specify axis (row=0, column=1)\n",
    "# need to specify inplace if you don't want to just create a copy\n",
    "df.drop('bar',axis=1,inplace=True)  # drop an old column\n",
    "print(df)\n",
    "\n",
    "# Or we can use del equivalently\n",
    "del df['baz']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- testing a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.DataFrame([[2, 3], [1, 50], [20, 4], [3, 45]])\n",
    "print(df)\n",
    "over40=(df>40)  # element-wise comparison\n",
    "print(over40)\n",
    "print(over40.any()) # any True in each column\n",
    "print(over40.any(axis=1))   # any True in each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Company Data example\n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = [[\"$229.2\", 2017, 123000, \"$1100\", \"Cupertino, US\"],\n",
    "     [\"$211.9\", 2017, 320671, \"$284\", \"Suwon, South Korea\"],\n",
    "     [\"$177.8\", 2017, 566000, \"$985\",  \"Seattle, US\"],\n",
    "     [\"$154.7\", 2017, 1300000, \"$66\", \"New Taipei City, Taiwan\"],\n",
    "     [\"$110.8\", 2017, 80110, \"$834\", \"Mountain View, US\"]]\n",
    "\n",
    "comps = [\"apple\", \"samsung\", \"amazon\", \"foxconn\", \"alphabet\"]\n",
    "cols = [\"revenue\", \"fy\", \"employees\", \"mcap\", \"location\"]\n",
    "\n",
    "c = pd.DataFrame(d, index=comps, columns=cols)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's how we look at just some of the data\n",
    "print(c.loc[['alphabet','amazon'],['location','revenue']])\n",
    "print(c.loc['amazon':'foxconn','revenue':'location'])\n",
    "print(c.iloc[2:4,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # remove the market cap column using del\n",
    "    del c['mcap']\n",
    "    # remove the fiscal year column using drop\n",
    "    c.drop('fy',axis=1,inplace=True)\n",
    "except KeyError as e:\n",
    "    pass\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a New Column Using a Dictionary\n",
    "c['state'] = pd.Series({'apple': 'CA', 'amazon': 'WA', 'alphabet': 'CA'})\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering Companies by Employee Size\n",
    "print(c[c['employees']<200000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting units\n",
    "print(c['employees']/100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "print(c,'\\n')\n",
    "print(c.index.tolist())\n",
    "print(list(c.index))\n",
    "print(c.index[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Companies with/without State Information\n",
    "print(c[c['state'].notnull()], '\\n')\n",
    "print(c[c['state'].isnull()], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling Missing State Data with empty strings in Pandas\n",
    "c['state']=c['state'].fillna('')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out just the revenue and employees columns for the those companies with a number of employees between 100,000 and 600,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.loc[(c['employees'] >= 100000) * (c['employees'] <= 600000),\n",
    "            ['revenue','employees']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized string methods on dataframe \n",
    "print(c['location'].str.upper(),'\\n')   # as Pandas Series, not in-place\n",
    "\n",
    "geosplit=c['location'].str.split(',')   # as Pandas Series, not in-place\n",
    "geosplit2=c['location'].str.split(',', expand=True)  # as Pandas DataFrame, not in-place\n",
    "print(geosplit,'\\n')\n",
    "\n",
    "c['country']=geosplit.str[-1]   # extract and add a new column `country`\n",
    "c['city']=geosplit.str[0]   # extract and add a new column `city`\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [str.slice](https://pandas.pydata.org/pandas-docs/version/2.1/reference/api/pandas.Series.str.slice.html#pandas.Series.str.slice) method (which is a vectorized string method to create substrings) and the the [astype(float)](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) method to convert the revenue column to a float by removing the dollar sign and converting the string to a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    c['revenue']=c['revenue'].str.slice(1).astype(float)\n",
    "    c['revenue']=c['revenue'].map(lambda x: float(x[1:])) # equivalent\n",
    "    c['revenue']=c['revenue'].str.replace('$', '', regex=False) # equivalent\n",
    "    # `regex=False` is needed to treat the dollar sign as a literal\n",
    "except AttributeError as e:\n",
    "    pass\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindexing/Rearranging a DataFrame\n",
    "# note the use of * here to unpack the elements of a list\n",
    "c=c.reindex(index=[*(list(c.index)[1:]), 'apple', 'microsoft'], # add Microsoft\n",
    "            columns=['city','state','country','revenue', 'employees'])\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Microsoft's Data using loc/iloc\n",
    "c.loc['microsoft','city']='Seattle'\n",
    "c.iloc[5,1]=\"WA\"\n",
    "c.iloc[5,2:]=[\"US\",161,182268]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City Rainfall example\n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "rain = pd.DataFrame([[3.50, 4.53, 4.13, 3.98],\n",
    "                     [7.91, 5.98, 6.10, 5.12],\n",
    "                     [3.94, 5.28, 3.90, 4.49],\n",
    "                     [1.42, 0.63, 0.75, 1.65]],\n",
    "    index=['New York', 'New Orleans', 'Atlanta', 'Seattle'],\n",
    "    columns=['Jun', 'Jul', 'Aug', 'Sept'])\n",
    "print(rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if axis is omitted, it is set to zero, i.e., sum across the rows for each column.\n",
    "total_rain_by_month=rain.apply(lambda x: sum(x)) \n",
    "print(total_rain_by_month)\n",
    "print(type(total_rain_by_month))\n",
    "print(rain.apply(np.sum)) # equivalent\n",
    "\n",
    "print('\\n', '-'*30, '\\n')\n",
    "\n",
    "# sum across the columns for each row\n",
    "total_rain=rain.apply(lambda x: sum(x), axis=1)\n",
    "print(total_rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Monthly Average Rainfall\n",
    "Average_rain=rain.apply(np.mean)\n",
    "print(Average_rain)\n",
    "\n",
    "print('\\n', '-'*30, '\\n')\n",
    "\n",
    "# Computing Average Rainfall for Each City\n",
    "Average_rain=rain.apply(np.mean,axis=1)\n",
    "print(Average_rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Monthly Rainfall Variability\n",
    "std=rain.apply(np.std)\n",
    "print(std)\n",
    "\n",
    "print('\\n', '-'*30, '\\n')\n",
    "\n",
    "# Rainfall Standard Deviation by City\n",
    "std=rain.apply(np.std, axis=1)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rain.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly Rainfall Descriptive Statistics by City\n",
    "print(rain.T.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting remark is that we see that mostly describe() method coincides with what we have calculated above using apply() method, except the std. The reason is that describe() by defualt use pd.Series.std() or pd.DataFrame.std() in the way of deviding n-1 instead of n to ensure the unbiased-ness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the minimum monthly rainfall by city and in which month it occurred.\n",
    "print(rain.min(axis=1))\n",
    "print('\\n', '-'*30, '\\n')\n",
    "print(rain.idxmin(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounding the Rainfall Data\n",
    "rain=rain.round(1)\n",
    "print(rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this doesn't make much sense in this context, but maybe they systematically undermeasured rainfall by 10 inches\n",
    "rain=rain.add(10).round(0)\n",
    "print(rain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert inches to cm in rainfall data   \n",
    "rain=rain.map(lambda x: 2.54*x)\n",
    "print(rain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "animals=['ant','cat','bat']\n",
    "animal=pd.Series(animals).map(lambda word: word + \"s\") \n",
    "print(animal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# apply factorial to the numbers in a 3x3 DataFrame containing the numbers from 0 to 8\n",
    "def factorial(n): \n",
    "    return 1 if n==0 else n*factorial(n-1)\n",
    "\n",
    "nums=pd.DataFrame(np.arange(9).reshape(3,3))\n",
    "print(nums)\n",
    "\n",
    "nums=nums.map(factorial)\n",
    "print(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Write a function which returns true if its argument is prime and false otherwise. Then use this function with map on a 4 by 4 dataframe containing the numbers 2 to 17 to return a dataframe of Boolean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def is_prime(n):\n",
    "    if n<2:\n",
    "        return False\n",
    "    for i in range(2, int(np.sqrt(n))+1):\n",
    "        if n%i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "df = pd.DataFrame(np.arange(2,18).reshape(4,4))\n",
    "\n",
    "print(df)\n",
    "print(df.map(is_prime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.DataFrame([['A.','c.'],['B.','d.']])\n",
    "df[0]=df[0].str.lower()\n",
    "df[1]=df[1].str.upper().str.replace('.','!')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation \n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)\n",
    "\n",
    "Check out this [book](https://stefvanbuuren.name/fimd/) concerning this topic! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# we create a dataframe containing some missing data.\n",
    "df = pd.DataFrame(np.arange(12).reshape((3, 4)),  columns=list('abcd'))\n",
    "# make two of the values missing\n",
    "df.loc[1, 'd'] = np.nan\n",
    "df.loc[2, 'c'] = np.nan\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One way is to drop rows or columns (often rude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.dropna(axis=0)   # rows\n",
    "print(df2)\n",
    "df3=df.dropna(axis=1)   # columns\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another way is to do mean imputation\n",
    "\n",
    "Mean imputation only works with numerical data, not categorical data. And it makes little sense for binary categorical data (like gender, for instance.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean imputation using dictionary\n",
    "df4=df.fillna({'c':df['c'].mean(),'d':df['d'].mean()})\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another way is to do [hot deck](https://pubmed.ncbi.nlm.nih.gov/21743766/) imputation \n",
    "\n",
    "Filling missing values with the previous non-missing value in the column. This can be reasonable if you expect similar observations (that is, rows) to be near one another in the list or if you have sorted them so that this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot deck imputation using forward fill\n",
    "df5=df.fillna(method='ffill')\n",
    "print(df5)\n",
    "# hot deck imputation using forward fill\n",
    "df6=df.fillna(method='bfill')\n",
    "print(df6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning\n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)\n",
    "\n",
    "The idea of categorizing continuous data into bins for easier analysis or reporting. Use `pd.cut(data,bins,labels)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "# generate 100 random numbers between 60 and 100 (simulating student grades)\n",
    "scores=np.random.randint(60,101,size=100)\n",
    "# represents 60-69 (D), 70-79 (C), 80-89 (B), 90-99 (A)\n",
    "# The bins are the boundaries of the bins as a list.\n",
    "bins=[59,69,79,89,100]\n",
    "# labels can be used to specify text labels for each bin.\n",
    "gradelabels=['D','C','B','A' ]  # ascending order\n",
    "print(scores)\n",
    "grades=pd.cut(scores,bins,labels=gradelabels)\n",
    "print(grades)\n",
    "print(type(grades))\n",
    "# This creates a categories object which can be counted with the value_counts method.\n",
    "print(pd.value_counts(grades,sort=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformatting\n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- converting strings to numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [['2009', '$500'],\n",
    "        ['2010', '$1,234'],\n",
    "        ['2011', 'bad data'],\n",
    "        ['2012', '$2,507']]\n",
    "df = pd.DataFrame(data , columns=['date', 'total'])\n",
    "print(df)\n",
    "\n",
    "df['total'] = df['total'].str.replace('$', '', regex=False)\n",
    "df['total'] = df['total'].str.replace(',', '',regex=False)\n",
    "# convert to numeric, coercing anything that doesn't convert\n",
    "df['total'] = pd.to_numeric(df['total'],errors='coerce')\n",
    "\n",
    "print(df)\n",
    "print(df.loc[2,'total'])\n",
    "print(type(df.loc[2,'total']))  # The nan is coerced to floating type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- converting strings to datetime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "s = pd.Series(['Jan 7, 2014', 'May 29, 1993'])\n",
    "s = pd.to_datetime(s)\n",
    "print(s, '\\n')\n",
    "# Numeric months\n",
    "print(s.dt.month, '\\n')\n",
    "# Month Names\n",
    "print(s.dt.month_name(), '\\n')\n",
    "# Numeric Week of the year\n",
    "print(s.dt.isocalendar().week, '\\n')\n",
    "# Day of the year\n",
    "print(s.dt.dayofyear, '\\n')\n",
    "# Day of the week\n",
    "print(s.dt.day_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging\n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'inner': intersection of keys from both DataFrames (default)\n",
    "- 'outer': union of keys from both DataFrames\n",
    "- 'left': include all keys from the first (left) DataFrame, even if they don't exist in second (right)\n",
    "- 'right': include all keys from the second (right) DataFrame, even if they don't exist in first (left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = pd.DataFrame([[2, 20], [4, 40], [6, 60], [8, 80]],\n",
    "                 columns=['k', 'col1'])\n",
    "b = pd.DataFrame([[4, 2], [4, 3], [8, 7], [10,6]],\n",
    "                 columns=['k', 'col2'])\n",
    "print(\"First Dataframe:\")\n",
    "print(a)\n",
    "print(\"Second Dataframe:\")\n",
    "print(b)\n",
    "print(\"Inner Join:\")    # default\n",
    "print(pd.merge(a,b,on='k'))\n",
    "# print(a.merge(b,on='k'))    # equivalent\n",
    "# print(b.merge(a,on='k'))    # equivalent\n",
    "print(\"Outer Join:\")\n",
    "print(pd.merge(a,b,on='k',how='outer'))\n",
    "print(\"Left Join:\")\n",
    "print(pd.merge(a,b,on='k',how='left'))\n",
    "print(\"Right Join:\")\n",
    "print(pd.merge(a,b,on='k',how='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- concatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "d1 = pd.DataFrame(np.arange(9).reshape((3, 3)),\n",
    "                columns=list('abc'))\n",
    "d2 = pd.DataFrame(np.arange(10, 19).reshape((3, 3)),\n",
    "                columns=list('abc'))\n",
    "print(d1)\n",
    "print(d2)\n",
    "d3=pd.concat([d1,d2])\n",
    "print(d3)\n",
    "d3.index=range(6) # resetting the index\n",
    "print(d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- merging exercise on `crossing` and `apply` method\n",
    "\n",
    "The pandas merge method, when called with the how=\"cross\" option, creates the Cartesian product of two dataframes, which combines each row in the first dataframe with each row in the second, without any key for matching (that is, every pair of rows matches.)\n",
    "\n",
    "Create a dataframe with one column called restaurant_name with values from the following list: [\"Amy's Pizza\", \"Beth's Pizza\", \"Charlie's Pizza\"].\n",
    "\n",
    "Create another dataframe with one column called slice_type with values from the following list: [\"plain\", \"mushroom\", \"anchovy\"].\n",
    "\n",
    "Then create a dataframe that is the Cartesian product of them, which should have nine rows. Create a new \"price\" column. Set the prices at Amy's as $\\$ 1$ for plain, $\\$ 1.25$ for mushroom, and $\\$ 1.50$ for anchovy. Then set all of the prices at Beth's at \\$1 more than Amy's. Then set the prices at Charlie's at \\$1 more than Beth's. Print the resultant dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # suppresses a warning\n",
    "\n",
    "df1 = pd.DataFrame(data=[\"Amy's Pizza\", \"Beth's Pizza\", \"Charlie's Pizza\"], columns=['Restaurant_Name'])\n",
    "df2 = pd.DataFrame(data=[\"plain\", \"mushroom\", \"anchovy\"], columns=['slice_type'])\n",
    "\n",
    "df3 = pd.merge(df1,df2,how='cross')   # Cartesian product\n",
    "\n",
    "print(df3, '\\n')\n",
    " \n",
    "base_prices = {\"plain\":1.00, \"mushroom\":1.25, \"anchovy\":1.50}\n",
    "increment = {\"Amy's Pizza\": 0.00, \"Beth's Pizza\":1.00, \"Charlie's Pizza\":2.00}\n",
    "\n",
    "df3['price']=df3.apply(lambda x: base_prices[x['slice_type']]+increment[x['Restaurant_Name']], axis=1)\n",
    "\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexes, Hierarchical Indexes, and Grouping\n",
    "\n",
    "through the US Department of Labor [data](https://www.bls.gov/oes/) example\n",
    "\n",
    "by `set_index` method\n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cols = [\"Year\", \"State\", \"Title\", \"Employment\", \"Salary\"]\n",
    "\n",
    "data = [[2016, \"CA\", \"Web Dev\", 22650, 82930],\n",
    "        [2016, \"CA\", \"DB Admin\", 12370, 93960],\n",
    "        [2016, \"NY\", \"Web Dev\", 11410, 81140],\n",
    "        [2016, \"NY\", \"DB Admin\", 6650, 91720],\n",
    "        [2017, \"CA\", \"Web Dev\", 21150, 84270],\n",
    "        [2017, \"CA\", \"DB Admin\", 12030, 95630],\n",
    "        [2017, \"NY\", \"Web Dev\", 11900, 82360],\n",
    "        [2017, \"NY\", \"DB Admin\", 7170, 94330],\n",
    "        [2018, \"CA\", \"Web Dev\", 20170, 86160],\n",
    "        [2018, \"CA\", \"DB Admin\", 10970, 100890],\n",
    "        [2018, \"NY\", \"Web Dev\", 12030, 79880],\n",
    "        [2018, \"NY\", \"DB Admin\", 7100, 99000]]\n",
    "\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = df.copy()\n",
    "df2 = df.reindex([4,3,2,1,0,12])  # reordering the rows, row 12 will be filled with Nan by default\n",
    "print(df2, '\\n')\n",
    "df_c.index=list(\"abcdefghijkl\")   # changing the index\n",
    "print(df_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using Year Column as the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df.set_index('Year')\n",
    "print(df3)\n",
    "\n",
    "try:\n",
    "  print(df3['Year'])\n",
    "except:\n",
    "  print(\"Year column is no longer there.It's now in the index.\")\n",
    "  print(\"The original index:\")\n",
    "  print(df.index) # original index\n",
    "  print(\"The new index:\")\n",
    "  print(df3.index) # new index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hierarchical Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the set_index method, you can specify more than one index, and this can create more than one level of indexing.\n",
    "df4=df.set_index(['Year','State'])\n",
    "print(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Data Using a Hierarchical Index\n",
    "print(df4.loc[2017], '\\n')\n",
    "print(df4.loc[2017,'CA'], '\\n')\n",
    "print(df4.loc[(2017,'CA'),'Title'], '\\n')\n",
    "print(df4.loc[(2017,'CA'),'Title'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `groupby` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = [\"Year\", \"State\", \"Title\", \"Employment\", \"Salary\"]\n",
    "data = [[2016, \"CA\", \"Web Dev\", 22650, 82930],\n",
    "        [2016, \"CA\", \"DB Admin\", 12370, 93960],\n",
    "        [2016, \"NY\", \"Web Dev\", 11410, 81140],\n",
    "        [2016, \"NY\", \"DB Admin\", 6650, 91720],\n",
    "        [2017, \"CA\", \"Web Dev\", 21150, 84270],\n",
    "        [2017, \"CA\", \"DB Admin\", 12030, 95630],\n",
    "        [2017, \"NY\", \"Web Dev\", 11900, 82360],\n",
    "        [2017, \"NY\", \"DB Admin\", 7170, 94330],\n",
    "        [2018, \"CA\", \"Web Dev\", 20170, 86160],\n",
    "        [2018, \"CA\", \"DB Admin\", 10970, 100890],\n",
    "        [2018, \"NY\", \"Web Dev\", 12030, 79880],\n",
    "        [2018, \"NY\", \"DB Admin\", 7100, 99000]]\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "\n",
    "# this groups the salaries by year and then takes their mean\n",
    "grouped=df[\"Salary\"].groupby(df['Year'])\n",
    "print(type(grouped))\n",
    "print(grouped.mean())\n",
    "print(grouped.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = [\"Year\", \"State\", \"Title\", \"Employment\", \"Salary\"]\n",
    "data = [[2016, \"CA\", \"Web Dev\", 22650, 82930],\n",
    "        [2016, \"CA\", \"DB Admin\", 12370, 93960],\n",
    "        [2016, \"NY\", \"Web Dev\", 11410, 81140],\n",
    "        [2016, \"NY\", \"DB Admin\", 6650, 91720],\n",
    "        [2017, \"CA\", \"Web Dev\", 21150, 84270],\n",
    "        [2017, \"CA\", \"DB Admin\", 12030, 95630],\n",
    "        [2017, \"NY\", \"Web Dev\", 11900, 82360],\n",
    "        [2017, \"NY\", \"DB Admin\", 7170, 94330],\n",
    "        [2018, \"CA\", \"Web Dev\", 20170, 86160],\n",
    "        [2018, \"CA\", \"DB Admin\", 10970, 100890],\n",
    "        [2018, \"NY\", \"Web Dev\", 12030, 79880],\n",
    "        [2018, \"NY\", \"DB Admin\", 7100, 99000]]\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "df=df.set_index(['Year','State'])\n",
    "# number of rows per state/year\n",
    "print(df.iloc[:,1:].groupby(\"State\").count())      # skipping the Title column\n",
    "print(df.iloc[:,1:].groupby(\"Year\").count())\n",
    "# mean salary per state/year\n",
    "print(df.iloc[:,1:].groupby(\"State\").mean())\n",
    "print(df.iloc[:,1:].groupby(\"Year\").mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=df.groupby('Year')['Salary'].mean()\n",
    "print(mean)\n",
    "\n",
    "max=df.groupby('Year')['Salary'].max()\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- exercise on grouping\n",
    "\n",
    "The file countries csv has been uploaded to workspace to the right. You can view it in one of the tabs to the right. It has the columns country, continent, gdpPerCapita2021, population, area, and gini.\n",
    "\n",
    "Use groupby to find the maximum per capita GDP for each continent, which countries have those maximums, and the average per capita GDP per continent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "countries=pd.read_csv(\"data/countries.csv\")\n",
    "\n",
    "grouped = countries.groupby(\"continent\")[\"gdpPerCapita2021\"]\n",
    "\n",
    "maximum = grouped.max()\n",
    "max_countries = countries.loc[grouped.idxmax()]['country']\n",
    "ave = grouped.mean()\n",
    "\n",
    "print(maximum)\n",
    "print(max_countries)\n",
    "print(ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following aggregation functions work with groupby in pandas.\n",
    "- count\n",
    "- sum, prod\n",
    "- mean, median\n",
    "- std, var\n",
    "- min, max\n",
    "- first, last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aggregation Exercise\n",
    "\n",
    "Write some code at the right to print the dataframe sorted by state and then by salary (in ascending order) and then print the second highest salary by state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "cols = [\"Year\", \"State\", \"Title\", \"Employment\", \"Salary\"]\n",
    "\n",
    "data = [[2016, \"CA\", \"Web Dev\", 22650, 82930],\n",
    "        [2016, \"CA\", \"DB Admin\", 12370, 93960],\n",
    "        [2016, \"NY\", \"Web Dev\", 11410, 81140],\n",
    "        [2016, \"NY\", \"DB Admin\", 6650, 91720],\n",
    "        [2017, \"CA\", \"Web Dev\", 21150, 84270],\n",
    "        [2017, \"CA\", \"DB Admin\", 12030, 95630],\n",
    "        [2017, \"NY\", \"Web Dev\", 11900, 82360],\n",
    "        [2017, \"NY\", \"DB Admin\", 7170, 94330],\n",
    "        [2018, \"CA\", \"Web Dev\", 20170, 86160],\n",
    "        [2018, \"CA\", \"DB Admin\", 10970, 100890],\n",
    "        [2018, \"NY\", \"Web Dev\", 12030, 79880],\n",
    "        [2018, \"NY\", \"DB Admin\", 7100, 99000]]\n",
    "\n",
    "df = pd.DataFrame(data, columns=cols)\n",
    "df_sorted = df.sort_values(by=[\"State\", \"Salary\"])\n",
    "print(df_sorted)\n",
    "\n",
    "f = lambda x: x.sort_values().iloc[-2]\n",
    "\n",
    "second_highest = df.groupby('State').aggregate({'Salary': f})\n",
    "\n",
    "print(second_highest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
